# codex-managed: keep aligned with deployment automation templates
# ─────────────────────────────────────────────────────────
# Core ports (may differ per stage)
API_PORT=9000
FRONTEND_PORT=3000

# ─────────────────────────────────────────────────────────
# Public API base used by the frontend (mounted under /lexi)
# Example: your Cloudflare/Nginx proxy forwards /lexi → backend
LEX_API_BASE_PUBLIC=https://api.lexicompanion.com/lexi

# ─────────────────────────────────────────────────────────
# Comfy endpoints — the backend (server-side) will talk to these.
# Prefer host.docker.internal (mapped via compose extra_hosts)
COMFY_URL=http://host.docker.internal:8188
COMFY_BASE_URL=http://host.docker.internal:8188
IMAGE_API_BASE=http://host.docker.internal:8188

# ─────────────────────────────────────────────────────────
# Models & assets
# Use a single base path and derive specific subpaths in code.
# Override BASE_MODELS_DIR per host/stage (examples below).
BASE_MODELS_DIR=/mnt/data/models
COMFY_WORKSPACE_DIR=${BASE_MODELS_DIR}/comfy
LEX_SDXL_CHECKPOINT=${COMFY_WORKSPACE_DIR}/sd/checkpoints/sdxl.safetensors

# ─────────────────────────────────────────────────────────
# LLM / OpenAI-compatible backends (example non-prod gateway)
OPENAI_API_BASE=http://host.docker.internal:8008/v1
OPENAI_API_KEY=dummy

# ─────────────────────────────────────────────────────────
# Logging
LEX_LOG_DIR=./logs/sessions

# ─────────────────────────────────────────────────────────
# NOTE:
# 1) Copy this file to .env.development and .env.production and adjust.
# 2) Real secrets belong ONLY in actual runtime env (.env files not committed),
#    Docker/Swarm secrets, or a vault — never in git.
